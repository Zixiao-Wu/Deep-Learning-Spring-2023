{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Individual Assignment 3: \n",
        "## Crowd Counting: Count the Number of Customers Appeared in the Business"
      ],
      "metadata": {
        "id": "CIoVhmS0LYF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Imgur](https://i.imgur.com/r1U9dHD.png) "
      ],
      "metadata": {
        "id": "GNS9x1pcUuuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Scenario**\n",
        "\n",
        ">\"We see our customers as invited guests to a party, and we are the hosts. Itâ€™s our job every day to make every important aspect of the customer experience a little bit better.\"   **--- Jeff Bezos, Amazon CEO**\n",
        "\n",
        "\n",
        "\n",
        "This is also true for all brick-and-motor stores at shopping malls. As a host, one thing you definitely should know is: *How many `guests` are attending your `party`*. \n",
        "\n",
        "The #(sign for quantity) of customers are curcial. Key factors for a successful business include `store rent`, `store sales`, which are all strongly correlated with `customers quantity`. \\\n",
        "\n",
        " \n",
        "\n",
        "In Practice, keeping track of passenger flow or customer traffic is never an easy thing. At the old times, customers are counted manually. Later, sensors are installed at the entrance, which provide a higher-accuracy solution, but suffers from high cost (Just imagine how many sensors should be installed). \\\n",
        "\n",
        "Not all malls have the luxury of installing sensors, but all malls have video surveillance cameras. With the images captured by real-time cameras, computer vision technology can help counting customers 24/7.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YP7NG6txPfj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/kaggle-datasets-images/526740/966025/1b806f39569b1157b1aaafe81f59f4b6/dataset-cover.jpg?t=2020-02-24-06-34-25\">"
      ],
      "metadata": {
        "id": "oJmLiXxsL2mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This step is done to connect the content directory to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eWOFA4hUk5e",
        "outputId": "e7fbea0e-7344-4e54-c285-6ba6b0d16e4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task**\n",
        "Build a neural network to count the number of customers appeared in the image captured so we can understand the potential customers.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wEJko_TaKWCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to download the dataset, please use below link. "
      ],
      "metadata": {
        "id": "AlpW9nv_VmRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link = 'https://drive.google.com/file/d/1RwuuNLpQWcozMOGw6Fp42m_dIhuqH90y/view?usp=sharing' # The shareable link\n"
      ],
      "metadata": {
        "id": "h5_0h4NBpYfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip the data file\n",
        "<font color=\"orange\"><b>Todo:</b></font>\\\n",
        "Replace the `PATH_TO_ZIP_FILE` with the path to your data file in your Google Drive. We are unzipping the data file to the `/content/crowd-counting` directory, which won't use your Google Drive storage."
      ],
      "metadata": {
        "id": "AYQ4mzCUjGa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/Colab Notebooks/deep learning/lab4/Crowd-Counting-Dataset.zip\" -d /content/crowd-counting # you should use your own link for the zip file"
      ],
      "metadata": {
        "id": "i5J44W3iEJ8a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load csv files and train-test-split\n",
        "<font color=\"orange\"><b>Todo:</b></font>\\\n",
        "(1) Read `/content/crowd-counting/labels.csv`: Read csv files as `Pandas DataFrame` into RAM\\\n",
        "(2) Use `Pandas` API, create a new column named `dir`, which stores the image file names. \\\n",
        "(3) Split the whole data into `training dataframe` (0.7) and `testing dataframe` (0.3)\\\n",
        "(4) Split the above `testing dataframe` into `testing dataframe`(0.5) and `validation dataframe` (0.5)\\\n",
        "Hint: Try to think about the relationship between the number above and the \"?\" parameters below."
      ],
      "metadata": {
        "id": "LbwYWP5dtW4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/crowd-counting/labels.csv\")\n",
        "X=\"dir\"\n",
        "Y=\"count\"\n",
        "df[X]=\"seq_\"+df.id.astype(str).str.zfill(6)+\".jpg\""
      ],
      "metadata": {
        "id": "RnZyjdxjVIZD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hHkHV_mO1OFb",
        "outputId": "07efb2c6-b6bc-43ca-f687-980e714a276d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  count             dir\n",
              "0   1     35  seq_000001.jpg\n",
              "1   2     41  seq_000002.jpg\n",
              "2   3     41  seq_000003.jpg\n",
              "3   4     44  seq_000004.jpg\n",
              "4   5     41  seq_000005.jpg"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58e44afc-83c7-406f-9503-5f40cb82fd77\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>count</th>\n",
              "      <th>dir</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>seq_000001.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>41</td>\n",
              "      <td>seq_000002.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>41</td>\n",
              "      <td>seq_000003.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>44</td>\n",
              "      <td>seq_000004.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>41</td>\n",
              "      <td>seq_000005.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58e44afc-83c7-406f-9503-5f40cb82fd77')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-58e44afc-83c7-406f-9503-5f40cb82fd77 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-58e44afc-83c7-406f-9503-5f40cb82fd77');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# Train-test-validation split\n",
        "train,_=train_test_split(df,test_size=0.3)\n",
        "validation,test=train_test_split(_,test_size=0.5)\n",
        "print(train.shape,validation.shape,test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx_oEr3PEyrQ",
        "outputId": "9505cbd8-2984-43f2-9954-04a2d1486937"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1400, 3) (300, 3) (300, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "Image.open(\"/content/crowd-counting/frames/frames/seq_000001.jpg\").mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3tSPj6o564xC",
        "outputId": "1366cb21-1543-4e13-9e30-63713c4de019"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RGB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick check for your answer:**\\\n",
        "(1400, 3) (300, 3) (300, 3)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QmK7VSMuy8Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure data generator\n",
        "<font color=\"orange\"><b>Todo:</b></font>\n",
        "\n",
        "Now, let's configure **3** generators: for training, validation, test respectively, in the code chunk below.\n",
        " \n",
        "\\\n",
        "**Note**: \\\n",
        "(1) Simply rescale input to `1./255`.\n",
        "You don't need to perform augmentations such as rotation, zooming, and so on. The reason is that the testing images is exactly the same as training images.\n",
        "\n",
        "(2) Here's a very helpful link about how to use the `flow_from_dataframe` API\\\n",
        "https://keras.io/api/preprocessing/image/"
      ],
      "metadata": {
        "id": "yyAvqUAc26jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "target_size = (120, 160)\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_gen = train_datagen.flow_from_dataframe(train,\n",
        "                                              directory='/content/crowd-counting/frames/frames',\n",
        "                                              target_size=target_size,\n",
        "                                              class_mode='raw',\n",
        "                                              x_col=X,\n",
        "                                              y_col=Y,\n",
        "                                              shuffle=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_gen = val_datagen.flow_from_dataframe(validation,\n",
        "                                          directory='/content/crowd-counting/frames/frames',\n",
        "                                          target_size=target_size,\n",
        "                                          class_mode='raw',\n",
        "                                          x_col=X,\n",
        "                                          y_col=Y)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_gen = test_datagen.flow_from_dataframe(test,\n",
        "                                            directory='/content/crowd-counting/frames/frames',\n",
        "                                            target_size=target_size,\n",
        "                                            class_mode='raw',\n",
        "                                            x_col=X,\n",
        "                                            y_col=Y,\n",
        "\n",
        "                                            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5BE0qmJqcJl",
        "outputId": "3010ce19-ade6-4278-d6e4-a2197ff4aab8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1400 validated image filenames.\n",
            "Found 300 validated image filenames.\n",
            "Found 300 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick check:**\\\n",
        "The result should be like: \\\n",
        "\n",
        "\n",
        "Found 1400 validated image filenames.\\\n",
        "Found 300 validated image filenames.\\\n",
        "Found 300 validated image filenames.\n"
      ],
      "metadata": {
        "id": "0iHD7xap7y5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Model\n",
        "<font color=\"orange\"><b>Todo:</b></font>\\\n",
        "Build your convolutional neural network sequential model. \n",
        "First, build a 2-hidden layer model. Then build a CNN model with 2 convolution layers. Your CNN model should outperform the dense layer model in order to get a full grade."
      ],
      "metadata": {
        "id": "TBISWTFf8vhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dense Layer Model"
      ],
      "metadata": {
        "id": "37z7XX1AThMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten,Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
        "model=Sequential()\n",
        "model.add(Flatten())  \n",
        "model.add(Dense(128 ,input_shape = (120*160*3,) ,activation = 'relu'))     \n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'relu'))\n",
        "\n",
        "\n",
        "model.compile(loss=\"mae\",                   \n",
        "              optimizer=\"adam\",\n",
        "              metrics=['mape'])\n",
        "\n",
        "\n",
        "#      # EarlyStopping with patience 2: if in 2 consecutive epochs, the validation loss didn't hit the record, training will stop.\n",
        "\n",
        "# Fit the model and record the training history for plotting purpose\n",
        "history=model.fit(train_gen,\n",
        "      steps_per_epoch=20, #train_gen.n//train_gen.batch_size. In lab 2, we set \"batchsize\" in model.fit() to determine the step length for steps in one epoch. \n",
        "                  #Steps_per_epoch is the number of steps in one epoch. It can also determine the step length (batchszie) in each epoch. So, we can set steps_per_epoch instead to replace \"batchsize\".\n",
        "                  #Either batchsize or steps_per_epoch is fine. Just make sure you set one of them.\n",
        "      epochs=20,\n",
        "      validation_data=val_gen,\n",
        "      validation_steps=10, #val_gen.n//val_gen.batch_size\n",
        "      )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3zaJm7InNoT",
        "outputId": "ddac0eb8-18cb-43cd-94fa-05dc13b42008"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 6s 221ms/step - loss: 8.8889 - mape: 30.0767 - val_loss: 6.2035 - val_mape: 20.7483\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 4s 217ms/step - loss: 6.0977 - mape: 21.0319 - val_loss: 7.0768 - val_mape: 21.4265\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 5s 245ms/step - loss: 6.2133 - mape: 20.9675 - val_loss: 7.0448 - val_mape: 26.9421\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 6s 319ms/step - loss: 6.0488 - mape: 20.6856 - val_loss: 6.1432 - val_mape: 18.7038\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 5s 230ms/step - loss: 5.5529 - mape: 18.4645 - val_loss: 5.5260 - val_mape: 16.8287\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 5s 248ms/step - loss: 5.2511 - mape: 17.5136 - val_loss: 7.2121 - val_mape: 20.9622\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 4s 212ms/step - loss: 5.0170 - mape: 17.0026 - val_loss: 4.3020 - val_mape: 14.0764\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 5s 269ms/step - loss: 4.6769 - mape: 15.5016 - val_loss: 4.0642 - val_mape: 12.9932\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 4s 213ms/step - loss: 6.0393 - mape: 20.0450 - val_loss: 5.5763 - val_mape: 20.9531\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 6s 287ms/step - loss: 5.3518 - mape: 18.4659 - val_loss: 8.7047 - val_mape: 26.0245\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 4s 212ms/step - loss: 4.7634 - mape: 16.1416 - val_loss: 3.8413 - val_mape: 11.9742\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 5s 273ms/step - loss: 3.7091 - mape: 12.6526 - val_loss: 3.2437 - val_mape: 10.8653\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 4s 212ms/step - loss: 3.6512 - mape: 12.5125 - val_loss: 3.1626 - val_mape: 10.0112\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 5s 278ms/step - loss: 4.0018 - mape: 13.7473 - val_loss: 3.3575 - val_mape: 11.7668\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 4s 218ms/step - loss: 4.2017 - mape: 14.4984 - val_loss: 3.6260 - val_mape: 11.3518\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 6s 291ms/step - loss: 3.6444 - mape: 12.0463 - val_loss: 3.3026 - val_mape: 11.5924\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 4s 217ms/step - loss: 4.0952 - mape: 13.5909 - val_loss: 7.2814 - val_mape: 22.5449\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 4s 216ms/step - loss: 4.4676 - mape: 14.6401 - val_loss: 3.9145 - val_mape: 14.4016\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 5s 270ms/step - loss: 3.3026 - mape: 11.0689 - val_loss: 3.0563 - val_mape: 9.7293\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 5s 261ms/step - loss: 3.0854 - mape: 10.3520 - val_loss: 3.1509 - val_mape: 11.0583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is provided for you to calculate model mse."
      ],
      "metadata": {
        "id": "e45GLjJI-gzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "y=[]\n",
        "for i in range(1+ test_gen.n//test_gen.batch_size):\n",
        "    y.extend(list(test_gen.next()[1]))\n",
        "\n",
        "predict=model.predict(test_gen)\n",
        "mean_absolute_error(predict.flatten(), y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omJbaC9GnNoT",
        "outputId": "18825a6d-1785-4217-c9d5-fce9e613f064"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 2s 215ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.409426809946696"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN Model"
      ],
      "metadata": {
        "id": "9jwfGR0iTr2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use any hyperparameters and improve your results here. "
      ],
      "metadata": {
        "id": "D199l868ZUXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(128, 3,input_shape=(120,160,3),activation=\"relu\"))\n",
        "model.add(MaxPooling2D(2))\n",
        "model.add(Conv2D(64, 3,activation=\"relu\"))\n",
        "model.add(MaxPooling2D(2))\n",
        "model.add(Flatten())                                                         # Add Dropout layer, randomly drops 50% neurons from propagation.\n",
        "model.add(Dense(64,activation=\"relu\", kernel_regularizer=\"l2\"))\n",
        "model.add(Dense(16,activation=\"relu\"))\n",
        "model.add(Dense(1,activation=\"relu\"))\n",
        "\n",
        "model.compile(loss=\"mae\",                   \n",
        "              optimizer=\"adam\",\n",
        "              metrics=['mape'])\n",
        "\n",
        "\n",
        "#      # EarlyStopping with patience 2: if in 2 consecutive epochs, the validation loss didn't hit the record, training will stop.\n",
        "\n",
        "# Fit the model and record the training history for plotting purpose\n",
        "history=model.fit(train_gen,\n",
        "      steps_per_epoch=20, #train_gen.n//train_gen.batch_size. In lab 2, we set \"batchsize\" in model.fit() to determine the step length for steps in one epoch. \n",
        "                  #Steps_per_epoch is the number of steps in one epoch. It can also determine the step length (batchszie) in each epoch. So, we can set steps_per_epoch instead to replace \"batchsize\".\n",
        "                  #Either batchsize or steps_per_epoch is fine. Just make sure you set one of them.\n",
        "      epochs=20,\n",
        "      validation_data=val_gen,\n",
        "      validation_steps=10, #val_gen.n//val_gen.batch_size\n",
        "      )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26j0m6L0nNoT",
        "outputId": "6b2d8116-4bc2-4744-8a99-37e93c850c14"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 14s 329ms/step - loss: 10.7510 - mape: 31.9391 - val_loss: 7.4940 - val_mape: 25.6271\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 6s 288ms/step - loss: 6.9609 - mape: 21.5017 - val_loss: 6.5480 - val_mape: 20.0105\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 5s 231ms/step - loss: 6.6697 - mape: 21.2535 - val_loss: 6.3997 - val_mape: 21.4551\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 6s 310ms/step - loss: 5.9247 - mape: 18.8142 - val_loss: 6.0502 - val_mape: 20.4705\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 5s 233ms/step - loss: 5.8012 - mape: 18.4227 - val_loss: 5.6010 - val_mape: 17.8399\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 5s 264ms/step - loss: 5.0759 - mape: 16.4219 - val_loss: 6.4008 - val_mape: 23.4750\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 5s 228ms/step - loss: 5.1053 - mape: 15.7079 - val_loss: 4.1818 - val_mape: 13.5609\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 6s 304ms/step - loss: 4.3515 - mape: 13.2008 - val_loss: 3.6589 - val_mape: 11.1824\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 5s 229ms/step - loss: 4.2720 - mape: 12.9389 - val_loss: 3.8871 - val_mape: 12.5430\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 6s 312ms/step - loss: 3.6080 - mape: 10.8579 - val_loss: 3.9190 - val_mape: 10.7092\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 6s 291ms/step - loss: 3.9807 - mape: 11.8564 - val_loss: 3.3508 - val_mape: 10.3677\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 5s 235ms/step - loss: 3.7486 - mape: 10.9900 - val_loss: 4.2352 - val_mape: 11.6906\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 6s 304ms/step - loss: 4.1521 - mape: 12.4689 - val_loss: 3.7224 - val_mape: 10.0974\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 5s 234ms/step - loss: 3.9360 - mape: 11.4077 - val_loss: 3.4936 - val_mape: 9.5807\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 5s 240ms/step - loss: 3.3431 - mape: 9.7245 - val_loss: 3.1554 - val_mape: 8.7199\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 6s 275ms/step - loss: 3.3799 - mape: 9.7791 - val_loss: 2.9541 - val_mape: 8.2824\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 5s 230ms/step - loss: 3.3291 - mape: 10.0371 - val_loss: 2.8466 - val_mape: 8.0083\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 6s 310ms/step - loss: 2.7659 - mape: 7.8751 - val_loss: 2.7815 - val_mape: 7.7974\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 5s 233ms/step - loss: 2.9140 - mape: 8.4927 - val_loss: 2.8391 - val_mape: 8.3912\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 5s 237ms/step - loss: 3.1104 - mape: 9.1396 - val_loss: 3.2774 - val_mape: 9.9839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "y=[]\n",
        "for i in range(1+ test_gen.n//test_gen.batch_size):\n",
        "    y.extend(list(test_gen.next()[1]))\n",
        "\n",
        "predict=model.predict(test_gen)\n",
        "mean_absolute_error(predict.flatten(), y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtPYXUoVnNoU",
        "outputId": "2a73ff6a-7701-42c7-c3c2-bb780509513a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 1s 136ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.029786243438721"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Conclusion and Discussion***"
      ],
      "metadata": {
        "id": "vkdlVp1cYIth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 1: Please explain which kinds of layer is used in your CNN model. What are the functions of those layers? Please explain and interpret your results.**\n",
        "\n"
      ],
      "metadata": {
        "id": "IfHqWPvIz3bA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<font color = blue> The model has these layers:<br>Conv2D layer: This is a convolutional layer that convolves the input image with 3x3 kernels and applies a ReLU activation function to the output for non-linear transformation. The layer has 128 output channels and produces an output size of (118, 158, 128). <br><br>MaxPooling2D layer: This is a pooling layer that downsamples the output of the Conv2D layer to reduce the spatial size of the feature maps, as well as the number of computations and parameters. The layer applies a 2x2 max pooling operation, taking the maximum value from each 2x2 region as output. The layer produces an output size of (59, 79, 128).<br><br>Conv2D layer: This is the second convolutional layer that convolves the output of the previous layer with 3x3 kernels and applies a ReLU activation function to the output for non-linear transformation. The layer has 64 output channels and produces an output size of (57, 77, 64). <br><br>MaxPooling2D layer: This is the second pooling layer that further downsamples the output of the second convolutional layer. The layer applies a 2x2 max pooling operation and produces an output size of (28, 38, 64).<br><br>Flatten layer: This is a flattening layer that flattens the output of the previous layer into a one-dimensional array for subsequent fully connected layers. The layer produces an output size of 68416.<br><br>Dense layer: This is a fully connected layer that maps the input vector to a hidden layer of size 64, applies a ReLU activation function to the output for non-linear transformation, and uses L2 regularization to constrain the model complexity and improve its generalization performance.<br><br>Dense layer: This is the second fully connected layer that maps the output of the previous layer to a hidden layer of size 16 and applies a ReLU activation function to the output for non-linear transformation.<br><br>Dense layer: This is the output layer that maps the output of the previous layer to a single node of size 1 and applies a ReLU activation function as the output activation function. The output of the layer represents the predicted number of people."
      ],
      "metadata": {
        "id": "7UT9XfPvYHDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 2: Which type of error measure did you use for the model training? How does this kind of error measure defined?** \n",
        "\n"
      ],
      "metadata": {
        "id": "RwAFk7dT02E8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<font color = blue> I used mean absolute error (MAE) as the error measure for model training. MAE is a regression loss function that measures the average absolute difference between the predicted values and the actual values. It calculates the mean of the absolute differences between the predicted and actual values, without considering their direction. In other words, MAE represents the average amount by which the predictions differ from the actual values."
      ],
      "metadata": {
        "id": "X2xrY_9vYCTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Question 3: What business problem can be addressed by CNN? Can you give an example?**"
      ],
      "metadata": {
        "id": "JH0pP_vRY8Nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<font color = blue>One business problem that can be addressed by CNNs is automatic image recognition and classification. For example, a retail company might want to use CNNs to automatically classify product images into different categories, such as clothing, electronics, or home appliances. By automating this process, the company can save time and resources that would otherwise be spent on manual classification. <br><br>Another example is in the healthcare industry, where CNNs can be used to analyze medical images, such as X-rays, MRIs, and CT scans, to identify and diagnose various diseases, such as cancer, heart disease, and neurological disorders. By using CNNs to automate the analysis process, healthcare professionals can save time and increase the accuracy of their diagnoses"
      ],
      "metadata": {
        "id": "GFnV45PYZGiu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fHqQWE0vHpD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
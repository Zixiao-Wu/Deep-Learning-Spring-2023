{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"M2.2. Optimizing the Learning of Neural Networks.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"thpR5jhuZibN"},"source":["# **DAT565E-Deep Learning for Prediction of Business Outcomes**"]},{"cell_type":"markdown","metadata":{"id":"SRz5n-T7ZkFc"},"source":["<div class=\"LI-profile-badge\"  data-version=\"v1\" data-size=\"large\" data-locale=\"en_US\" data-type=\"horizontal\" data-theme=\"light\" data-vanity=\"drsalihtutun\"><a class=\"LI-simple-link\" href='https://www.linkedin.com/in/drsalihtutun/en-us?trk=profile-badge'>Salih Tutun, PhD</a></div>"]},{"cell_type":"markdown","metadata":{"id":"Z3mYjaJVZpD_"},"source":["![Imgur](https://i.imgur.com/4HJknC2.png)"]},{"cell_type":"code","metadata":{"id":"dHi-iT7U_0ny","executionInfo":{"status":"ok","timestamp":1595798417947,"user_tz":300,"elapsed":2080,"user":{"displayName":"Salih Tutun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHG_qVnuJVr4yVK7YY52UQNVj-LsW44YwRmRztJA=s64","userId":"02520511273410275582"}},"outputId":"a135a9c2-741f-438e-ab73-c060f35d7521","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["import keras\n","keras.__version__"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.3.1'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"MpnV-mnTxkXa"},"source":["### Data Vanishing and Exploding Gradients"]},{"cell_type":"markdown","metadata":{"id":"VkB-KDe4mfBo"},"source":["- Derivatives or slopes are getting exponentially big or small.\n","- If initial weights are less than 1, it decreases exponentially.\n","- If initial weights are more than 1, it increases exponentially.\n","- It takes a long time for gradient descent to learn.\n","- This makes training difficult."]},{"cell_type":"markdown","metadata":{"id":"f_LeEdRMxsOa"},"source":["![Imgur](https://i.imgur.com/3DA3ObZ.png)"]},{"cell_type":"markdown","metadata":{"id":"qa8KcNQFpRxo"},"source":["For training our neural networks faster, "]},{"cell_type":"markdown","metadata":{"id":"AU1P252v-Ykd"},"source":["### Mini Batch Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"-52a5sdvpkEq"},"source":["- We do not want to use gradient descent for all training set because it is slow\n","- Split the training set as small sets"]},{"cell_type":"markdown","metadata":{"id":"jlmFc7L2-chU"},"source":["![Imgur](https://i.imgur.com/iGMhz1P.png)"]},{"cell_type":"markdown","metadata":{"id":"SGlRmYOs_a9M"},"source":["### Exponentially weighted moving averages"]},{"cell_type":"markdown","metadata":{"id":"ccx4fRFUrgm1"},"source":["- Every time, we average the value with the previous one and present one.\n","- We can average 1/(1-beta) hours for stock price"]},{"cell_type":"markdown","metadata":{"id":"NEup8K-k_bzR"},"source":["![Imgur](https://i.imgur.com/Yokh435.png)"]},{"cell_type":"markdown","metadata":{"id":"J0523b5AV4g-"},"source":["### Gradient Descent with Momentum"]},{"cell_type":"markdown","metadata":{"id":"0IjTep35V9LR"},"source":["![Imgur](https://i.imgur.com/P4nepyP.png)"]},{"cell_type":"markdown","metadata":{"id":"zC7mmipYbihJ"},"source":["### RMSprob "]},{"cell_type":"markdown","metadata":{"id":"2txEU2uNbjBZ"},"source":["![Imgur](https://i.imgur.com/vTnaZPq.png)"]},{"cell_type":"markdown","metadata":{"id":"i3zTm7vCcIWg"},"source":["### Adaptive Momentum (Adam) Optimization"]},{"cell_type":"markdown","metadata":{"id":"OGsl9oyLunn9"},"source":["- RMSprob and Momentum together"]},{"cell_type":"markdown","metadata":{"id":"6NvuqBTecLvP"},"source":["![Imgur](https://i.imgur.com/XBlRO0I.png)"]},{"cell_type":"markdown","metadata":{"id":"fM0IxJ09g2Sz"},"source":["### Learning Rate Decay"]},{"cell_type":"markdown","metadata":{"id":"nZEOuydKvcM8"},"source":["We slowly reduce the learning rate"]},{"cell_type":"markdown","metadata":{"id":"_kgQO1uYg5R3"},"source":["![Imgur](https://i.imgur.com/ZqF4NCm.png)"]},{"cell_type":"markdown","metadata":{"id":"aytzbi2xoD1e"},"source":["### Batch Normalization"]},{"cell_type":"markdown","metadata":{"id":"MRLfVWjBoGH6"},"source":["- We normalize the input before.\n","- Normalize Z before the activation function\n","- Scaling all Z values as mean=0 and variance 1. \n","- It helps as regularization in the model. \n"]},{"cell_type":"markdown","metadata":{"id":"NlW7mG8mq3X6"},"source":["![Imgur](https://i.imgur.com/kpWwA40.png)"]},{"cell_type":"markdown","metadata":{"id":"vy3CAA6VujDI"},"source":["### Sofmax Regression"]},{"cell_type":"markdown","metadata":{"id":"W-8DFHiRulXS"},"source":["![Imgur](https://i.imgur.com/bkXovQX.png)"]},{"cell_type":"markdown","metadata":{"id":"wcDGStZIzQSO"},"source":["If you have questions, please contact with me.\n","\n","Salih Tutun, PhD\n","\n","salihtutun@wustl.edu"]}]}